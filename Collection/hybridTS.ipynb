{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components and Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that we can design effective hybrids, we need a better understanding of how time series are constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three patterns of dependence: trend, seasons, and cycles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many time series can be closely described by an additive model of just these three components plus some essentially unpredictable, entirely random error:\n",
    "- > series = trend + seasons + cycles + error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the terms in this model we would then call a component of the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals of a model are the difference between the target the model was trained on and the predictions the model makes -- the difference between the actual curve and the fitted curve, in other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could imagine learning the components of a time series as an iterative process: \n",
    "- > first learn the trend and subtract it out from the series, \n",
    "- > then learn the seasonality from the detrended residuals and subtract the seasons out, \n",
    "- > then learn the cycles and subtract the cycles out, \n",
    "- > and finally only the unpredictable error remains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add together all the components we learned and we get the complete model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially what linear regression would do if you trained it on a complete set of features modeling trend, seasons, and cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Forecasting with Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one algorithm to fit the original series and then the second algorithm to fit the residual series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing Hybrids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are generally two ways a regression algorithm can make predictions: either by transforming the features or by transforming the target. \n",
    "- Feature-transforming algorithms learn some mathematical function that takes features as an input and then combines and transforms them to produce an output that matches the target values in the training set. \n",
    "\n",
    "   > Linear regression and neural nets are of this kind.\n",
    "- Target-transforming algorithms use the features to group the target values in the training set and make predictions by averaging values in a group; a set of feature just indicates which group to average. \n",
    "\n",
    "    > Decision trees and nearest neighbors are of this kind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - US Retail Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['Month', 'BuildingMaterials', 'FoodAndBeverage']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m data_dir \u001b[39m=\u001b[39m Path(\u001b[39m\"\u001b[39m\u001b[39mE:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCoding\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mMachineLearning\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mCSVFile\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m industries \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mBuildingMaterials\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFoodAndBeverage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m retail \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\n\u001b[0;32m     41\u001b[0m     data_dir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mus-retail-sales.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     42\u001b[0m     usecols\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mMonth\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m+\u001b[39;49m industries,\n\u001b[0;32m     43\u001b[0m     parse_dates\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mMonth\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     44\u001b[0m     index_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMonth\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     45\u001b[0m )\u001b[39m.\u001b[39mto_period(\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreindex(columns\u001b[39m=\u001b[39mindustries)\n\u001b[0;32m     46\u001b[0m retail \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat({\u001b[39m'\u001b[39m\u001b[39mSales\u001b[39m\u001b[39m'\u001b[39m: retail}, names\u001b[39m=\u001b[39m[\u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mIndustries\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m retail\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1753\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1753\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n\u001b[0;32m   1754\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1755\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:135\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mset\u001b[39m(usecols)\u001b[39m.\u001b[39missubset(\n\u001b[0;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_names\n\u001b[0;32m    134\u001b[0m ):\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_usecols_names(usecols, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_names)\n\u001b[0;32m    137\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames) \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(usecols):  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:917\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    915\u001b[0m missing \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m usecols \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m names]\n\u001b[0;32m    916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    919\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m     )\n\u001b[0;32m    922\u001b[0m \u001b[39mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['Month', 'BuildingMaterials', 'FoodAndBeverage']"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from warnings import simplefilter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\n",
    "    \"figure\",\n",
    "    autolayout=True,\n",
    "    figsize=(11, 4),\n",
    "    titlesize=18,\n",
    "    titleweight='bold',\n",
    ")\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=16,\n",
    "    titlepad=10,\n",
    ")\n",
    "plot_params = dict(\n",
    "    color=\"0.75\",\n",
    "    style=\".-\",\n",
    "    markeredgecolor=\"0.25\",\n",
    "    markerfacecolor=\"0.25\",\n",
    ")\n",
    "\n",
    "data_dir = Path(\"E:\\\\Coding\\\\MachineLearning\\\\CSVFile\")\n",
    "industries = [\"BuildingMaterials\", \"FoodAndBeverage\"]\n",
    "retail = pd.read_csv(\n",
    "    data_dir / \"us-retail-sales.csv\",\n",
    "    usecols=['Month'] + industries,\n",
    "    parse_dates=['Month'],\n",
    "    index_col='Month',\n",
    ").to_period('D').reindex(columns=industries)\n",
    "retail = pd.concat({'Sales': retail}, names=[None, 'Industries'], axis=1)\n",
    "\n",
    "retail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's use a linear regression model to learn the trend in each series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration, we'll use a quadratic (order 2) trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = retail.copy()\n",
    "\n",
    "# Create trend features\n",
    "dp = DeterministicProcess(\n",
    "    index=y.index,  # dates from the training data\n",
    "    constant=True,  # the intercept\n",
    "    order=2,        # quadratic trend\n",
    "    drop=True,      # drop terms to avoid collinearity\n",
    ")\n",
    "X = dp.in_sample()  # features for the training data\n",
    "\n",
    "# Test on the years 2016-2019. It will be easier for us later if we\n",
    "# split the date index instead of the dataframe directly.\n",
    "idx_train, idx_test = train_test_split(\n",
    "    y.index, test_size=12 * 4, shuffle=False,\n",
    ")\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "\n",
    "# Fit trend model\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_fit = pd.DataFrame(\n",
    "    model.predict(X_train),\n",
    "    index=y_train.index,\n",
    "    columns=y_train.columns,\n",
    ")\n",
    "y_pred = pd.DataFrame(\n",
    "    model.predict(X_test),\n",
    "    index=y_test.index,\n",
    "    columns=y_test.columns,\n",
    ")\n",
    "\n",
    "# Plot\n",
    "axs = y_train.plot(color='0.25', subplots=True, sharex=True)\n",
    "axs = y_test.plot(color='0.25', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_fit.plot(color='C0', subplots=True, sharex=True, ax=axs)\n",
    "axs = y_pred.plot(color='C3', subplots=True, sharex=True, ax=axs)\n",
    "for ax in axs: ax.legend([])\n",
    "_ = plt.suptitle(\"Trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the linear regression algorithm is capable of multi-output regression, the XGBoost algorithm is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict multiple series at once with XGBoost, we'll instead convert these series from wide format, with one time series per column, to long format, with series indexed by categories along rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `stack` method converts column labels to row labels, pivoting from wide format to long\n",
    "X = retail.stack()  # pivot dataset wide to long\n",
    "display(X.head())\n",
    "y = X.pop('Sales')  # grab target series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that XGBoost can learn to distinguish our two time series, we'll turn the row labels for 'Industries' into a categorical feature with a label encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create a feature for annual seasonality by pulling the month numbers out of the time index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn row labels into categorical feature columns with a label encoding\n",
    "X = X.reset_index('Industries')\n",
    "# Label encoding for 'Industries' feature\n",
    "for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# Label encoding for annual seasonality\n",
    "X[\"Month\"] = X.index.month  # values are 1, 2, ..., 12\n",
    "\n",
    "# Create splits\n",
    "X_train, X_test = X.loc[idx_train, :], X.loc[idx_test, :]\n",
    "y_train, y_test = y.loc[idx_train], y.loc[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll convert the trend predictions made earlier to long format and then subtract them from the original series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That will give us detrended (residual) series that XGBoost can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot wide to long (stack) and convert DataFrame to Series (squeeze)\n",
    "y_fit = y_fit.stack().squeeze()    # trend from training set\n",
    "y_pred = y_pred.stack().squeeze()  # trend from test set\n",
    "\n",
    "# Create residuals (the collection of detrended series) from the training set\n",
    "y_resid = y_train - y_fit\n",
    "\n",
    "# Train XGBoost on the residuals\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_resid)\n",
    "\n",
    "# Add the predicted residuals onto the predicted trends\n",
    "y_fit_boosted = xgb.predict(X_train) + y_fit\n",
    "y_pred_boosted = xgb.predict(X_test) + y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit appears quite good, though we can see how the trend learned by XGBoost is only as good as the trend learned by the linear regression -- in particular, XGBoost wasn't able to compensate for the poorly fit trend in the 'BuildingMaterials' series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs = y_train.unstack(['Industries']).plot(\n",
    "    color='0.25', figsize=(11, 5), subplots=True, sharex=True,\n",
    "    title=['BuildingMaterials', 'FoodAndBeverage'],\n",
    ")\n",
    "axs = y_test.unstack(['Industries']).plot(\n",
    "    color='0.25', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_fit_boosted.unstack(['Industries']).plot(\n",
    "    color='C0', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "axs = y_pred_boosted.unstack(['Industries']).plot(\n",
    "    color='C3', subplots=True, sharex=True, ax=axs,\n",
    ")\n",
    "for ax in axs: ax.legend([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e593ac106456af50ce7af38f9671c411b49d6cd90f9b885e167f0f594e09038c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
